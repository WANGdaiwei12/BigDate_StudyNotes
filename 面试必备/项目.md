[TOC]

##### 个人介绍

太原理工大学  计算机科学与技术  2020/09 - 2023/05   网教 函授的  自学的  

业务数据，

你好，我叫王聪伟，今年25岁，有三年大数据开发经验，上家公司昱新科技有限公司，主要是做数据开发，技术服务这一方面的，我在公司主要是负责数据etl清洗，数据分析系统的维护，以及指标分析，在职期间，接触过直播类型的数据分析，短视频数据统计，app数据分析之类的项目。我个人比较擅长的模块就是hive离线数据分析这块，spark也会。就是我的个人情况了。

北京昱新科技有限公司      北京市朝阳区安外小关北里43号渔阳置业大厦

北街家园  沙河高教园  -昌平线 -清河站- 13号线-知春路地铁 - 10号线内环 -  安贞门地铁站 b

网络直播数据分析（2021.9-2022.4），西瓜视频（2021.5-2021.9），掌阅app（2020.11-2021.4）

2019-07 至 2020-10 北京阳光巨人教育科技有限公司  
地址 海淀区中关村大街49号兴发大厦

阳光巨人教育实时报表（2019-07）     阳光巨人教育线上用户分析（2020-05-2020-10）

部门：财务部，行政部，人力资源部，产品部，销售部 互联网事业部
互联网事业部 ,爬虫组 8,bi组 7,投研组： 3    数据分析师  3 

离职原因：1.项目空档期，想看看有没有新的机会，另一方面公司经营效益不太好吗，

职业规划：
	主攻方向大数据领域 ，我最近在学习实时的技术，计划向实时方面的发展，
	未来的话，尽自己的努力当一名大数据架构师或项目经理

公司介绍： 为金融、泛消费等行业的企业提供数字化运营解决方案的服务商，同时有提供数据开发，数据支持依靠大数据，AI等技术手段，为企业建立“新型数据银行”、并提供新一代智能数据分析平台
公司主营业务：
	针对现有的直播平台，进行数据分析，有以下平台：
	虎牙,欢聚时代,哔哩哔哩,斗鱼,淘宝,BIGO,QQ音乐,酷我音乐.全民K歌,荔枝,喜马拉雅,网易云音乐.

数据用处：为股票平台提供数据走势
	为对手公司提供数据支持
	为电商平台提供数据支持，促进用户的消费，有利于广告精准投放

主要职责：参与公司需求制定 
	与架构师共同搭建公司分析系统和日常维护
	对项目中的难点进行讨论研究
	使用sql完成公司的需求

每天干啥：  1.开会指定小的需求  以及每天的任务          看bug平台     2.拉取分支上最新的内容        3. 测试部门进行测试，          4.提交代码

##### 项目流程：

```tex
Apache集群、CDH集群
CDH：生产集群： 15台机器：8核16G内存
Apache：9台： 2核4G 主要用于数据存储 磁盘大 9T
200g   50g  活跃用户有  20万
Hadoop2.7.2 + Zookeeper3.4.7 + Hive1.2 + Kafka0.9 + Flume1.8 + Azkaban2.5.0 + Sqoop1.4.6 + MySql5.5.4
我们的数据仓库主要是基于维度建模，主要使用星形模型，
建表是指定row format serde ‘org.openx.data.jsonserde.JsonSerDe’这个类来对json类型的数据进行解析
我们这个项目主要对目前主流的直播平台，（比如虎牙，哔哩哔哩，yy，斗鱼，映客，酷狗，淘宝）进行一个数据统计分析，我主要扶着，虎牙和yy的数据统计，  200g  去重之后 50g
以虎牙为例，分成4个模块，公司概况、主播分析、内容分析、top主播跟踪，其实对应的是报表展示页面，
从天，周,月三个大维度来进行分析统计，
公司概况的话，我们会统计头部粉丝贡献礼物收入(万)、活跃主播数 
内容分析方面的话，我们会从游戏和娱乐两个方面，分别统计它们的礼物总收入占比，以及统计娱乐类中星秀，吃喝玩乐，颜值的礼物收入占比，游戏类的话统计英雄联盟、王者荣耀，绝地求生、穿越火线的礼物收入占比，
主播分析的话，我们会统计top10，top50，top100的主播礼物收入占比，主播分位值，以及礼物大于10万，
top主播跟踪的话，我们主要统计上季度的礼物打赏排名，主播名，类别，收入


首先爬虫工程师将数据放在本地，我使用flume
这个项目我主要负责将爬虫工程师爬取存入到磁盘的数据利用flume+kafka
传输到Hdfs上，将数据从HDFS上存入到数仓的ods层，按时间日期创建分区表，防止后续的全表扫描，我们还会使用orc列式存储，同时采用lzo对数据进行压缩，减少存储空间，

dwd层则是我们的一个数据过滤层，这一层我们主要对数据进行清洗，过滤掉控制，脏数据，重复数据，并且根据类别进行分类，过滤我们的条件就是，批次号=分区时间，并且 uid的长度>0,平台长度>0 去重的话，使用开窗函数row——number，根据platform，uid，批次号分区，按抓取时间排序，倒叙取第一，分类的话使用case when 函数 判断类型为刺激战场，和平精英，全军出击都给他定义为游戏类的绝地求生，娱乐类的话，比如户外，美食，音乐都定义它为娱乐类的吃喝玩乐，存储的类型是parquet，
从dwd层开始

dwi层数据汇总层，主要对数据进行进行一个轻度的聚合，生成一系列的中间表，提升公共指标的复用性，减少重复加工。比如 我们会求，月度，周度以及日度的晚高峰人气均值，，头部粉丝贡献礼物收入，
    比如 我们需要统计游戏和娱乐的晚高峰人气均值 有的主播可能是多个类别，那么我们就需要考虑类型，我们会取每个主播7天内出现最多	的分类进行统计 从dwd直播贡献表查数据这周的的主播id，平台，贡献值，左连接子查询，这个子查询是查（通过查询dwd的直播基础数据表	获取7天内出现	最多的类别，人气均值，最大值，最小值，使用的是avg(one_line),条件是 substring(batch-tm,12,2) in (18-24)，
 dws数据服务层，面向主体，建立宽表（直播平台股票分析信息表），字段有,统计日期
    直播平台，礼物总收入，活跃主播数，新增主播数，top10主播收入占比，top10主播收入，收入大于5万的主播数，分类占比，游戏top100	用户贡献值，娱乐top100用户贡献值等

    比如娱乐，游戏的营收值revenue， 根据dwi的日度表查询出主播id，平台，类型，营收 条件是营收值>0 并且dc='week' dt = 		'partition_dt',根据平台和类型分组，
    活跃主播数：count(uid) 从dwi日度表查询主播id 条件是 where dt = {partition_dt}，
    每个分类的活跃主播数：从dwi的日度表查询出主播id，平台，类型， 条件是dt = {partition_dt} 按类型live_content_tag分组

app层数据应用层，直接对应需求，将查出来的结果导入mysql中
通过dws层宽表获取到个模块对应的指标数据，然后通过sqoop迁移到mysql中
	
```

##### flume作用 kafka作用

```tex
自定义拦截器：定义一个类 实现Interceptor接口，实现intercept方法；然后打成jar包，放到flume的lib目录下，
在配置文件中定义拦截器：a1.sources.r1.interceptors=i1,a1.sources.r1.interceptors.i1.type=类的全路径名
$Builder


flume1.8版本，采集数据的效果比较好，flume的传输有事务，可以防止数据丢失，其次，对hdfs和kafka支持也比较好，而且可以调节写入目的的存储速率，减少hdfs的压力，我们还会自定义拦截器，对收集到的日志做了层过滤，因为有的数据没有uid 或一些关键字段,
kafka0.11,的作用：1.使各层解耦，消峰，异步处理，数据一方面可以同步到hdfs，一方面做实时计算，实现数据多分发
```

##### 项目优化：

```tex
flume优化：调整Flume进程的内存大小，太小的话会导致GC频繁
调整flume-env.sh脚本中的JAVA_OPTS参数
export JAVA_OPTS="-Xms1024m -Xmx1024m -Dcom.sun.management.jmxremote"
这里的Xms和Xmx设置为一样大，避免进行内存交换，内存交换也比较消耗性能。
```

##### 项目难点

```tex
1.理解业务   头部粉丝贡献礼物收入在一个fan_info  就是当天的一个主播前50的粉丝贡献值累加，使用get_json_object 获取贡献值的字段，然后使用 later view explode 炸裂函数进行一列转成多行，然后，使用row_number,over()函数对排名前50的数据进行一个累加。
3使用动态分区，分区不识别，造成指定分区的数据查不到，使用msck pepir table 修复分区
4.sqoop迁移数据  报IOException:     Incorrect string value '\xF0\x9F   '
原因 数据中有emoji表情  用4个字节存储的，而数据库使用的是utf8只能存储1-3个字节的字符
设置库的字符集为utf8mb4
解决方法：在url   jdbc:mysql://{host}/{db}?characterEncoding=utf8&autoReconnect=true
sqoop参数
/opt/module/sqoop/bin/sqoop export \
--connect \    连接地址
--username \    
--password \
--table  \ mysql的表
--input-fields-terminated-by "," \  指定mysql按什么进行切割
--export-dir \ hive上的要导出路径
--delete-target-dir \
--num-mappers \    启动n个map并行导入数据 
 --fields-terminated-by '\t'   \  指定分隔符

```

##### 数据库和数据仓库的区别

```tex
1.数据仓库其实是⼀种特殊的数据库它使用时多维模型，擅长OLAP，⼤数据量查询分析，数据加⼯，存储以及反映数据历史变化的，主要为公司的决策提供数据支持。⽽传统数据
库遵从范式模型（1NF，2NF，3NF，等等），从⽽尽可能减少数据冗余，加擅长事务处理OLTP，增删改查。

```

##### 数据仓库的理解，以及分层的理解

```tex
数据仓库是一个面向主体，集成的，反应历史状态的数据库，可以理解成它是一整套包括了etl、调度、建模在内的完整的理论体系，主要为公司的决策提供数据支持的，更擅长数据分析olap

分层：
以空间换时间：可以复用之前一层已经计算的结果，减少极大的重复计算。
把复杂的问题简单化：将一个复杂的任务分解成多个步骤来完成，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复；
便于处理业务的变化：随着业务的变化，只需要调整底层的数据，对应用层对业务的调整不受影响；
```

##### 数仓建模阶段，数仓建模的方法

```tex
建模阶段：
	1.业务建模，生成业务模型，
1.范式建模法
	1.表中的每个字段必须唯一，2.，满足第一范式基础，非主键的字段必须要完全依赖主键，不能依赖主键的一部分
	3.非主键的字段不能依赖于其他关系的属性，避免传递依赖，
2.维度建模法
	星型模型，由一个实时表和多个维度表
	雪花模型：
	星座模型：
3.实体建模法
```



字段 ods直播详情表(id     ,uid   '主播号,cat1   '一级分类,cat2  '二级分类',platform    '平台 ,fans  '粉丝数',crawl_time  抓取时间 ，online       '在线人数',contribution      '贡献值',batch  '批次号,nickname    '昵称'
  ,fans_info          '粉丝信息',replay        '是否重播 1 重播,0 直播'）

ods虎牙直播贡献信息表（）

dwd 虎牙直播基础信息表，直播贡献表，

dwi 虎牙主播信息日度表（主播号，平台，昵称，内容分类，一级分类，二级分类，粉丝数，晚高峰人气均值，头部粉丝贡献礼物收入，营收值）（dc 时间类型，dc 分区类型（第几周，第几个月），dt 分区时间，）

周度，月度的

dws虎牙直播平台股票分析信息表(直播平台，礼物总收入，活跃主播数，新增主播数，top10主播占比，top10主播收入，)

收入大于5万的主播数量，分类占比）

APP层，一个模块一张表



西瓜视频：视频详情表

item_id   '视频id',title   '标题',uid  '用户id',nickname   '用户昵称',user_desc    '认证信息',user_signature     '简介',user_video_count   '用户视频数',video_watch_count    '观看次数',share_count      '分享次数',comment_count       '评论数',video_duration      '视频时长',categories    '分类',digg_count      '点赞数',video_uri      '视频url'

西瓜用户详情表（uid      '用户id',name     '昵称',video_total_count    '视频数',video_total_play_count    '总播放量',video_total_series_count    '视频所属类别',video_total_share_count  '视频分享次数',user_verified       '是否是认证用户',user_auth_info   '认证信息',followers_count     '粉丝数',followers_count_str        string '粉丝信息',desc                      string comment '简介',crawl_time                 string comment ''
,batch_time   '批次时间'        ）









北京阳光巨人教育科技有限公司
地址 海淀区中关村大街49号兴发大厦

公司主营业务
博学家园小区 7号 703  高青路地铁站2号口  世纪大道地铁站12






​	



在公司主要做过的项目，我个人比较擅长hadoop，hive，spark等大数据分析组件，。，，对于hadoop衍生出的一系列组件kafak，zookeeper，可以熟练使用

 多个维度，我主要扶着，虎牙和yy的  200g  去重之后 50g



难点：

1.理解业务   头部粉丝贡献礼物收入同比     
2dws层   mysql迁移的   多表连接查询，导致oom     任务调度
3flume采集数据重复,flume有事务机制，宕机重启会导致数据重新传输，
4.sqoop迁移数据  报IOException:     Incorrect string value '\xF0\x9F   '
原因 数据中有emoji表情  用4个字节存储的，而数据库使用的是utf8只能存储1-3个字节的字符
设置库的字符集为utf8mb4
解决方法：在url   jdbc:mysql://{host}/{db}?characterEncoding=utf8&autoReconnect=true

3

历史表     唯一主键  id  平台    昵称  注册时间 ，抓取时间


详情主播信息  ods_spider_invs_hive_detail
直播贡献表：ods_spider_invs_hive_income

dwd：
直播基础表：dwd_investrch_spider_zhibo_detail;
直播贡献表：dwd_zhibo_income_info
dwi
主播日度信息表 :dwi_zhibo_day_info
主播周度信息表：dwi_zhibo_week_info
主播月度信息表：dwi_zhibo_month_info


宽表：'直播平台股票分析信息表  dws_zhibo_stock_analyze_info
platform_type string,type_info string,dc string,dt string
平台类型             指定时间的类型，   指定时间


select case when substr(tags,1,2)=id then lab
 when substr(tags,3,4)=id then lab
 when substr(tags,5,6)=id then lab
from t1 ,t2


bin/sqoop export \
--connect   'jdbc:mysql://192.167.43.2:3306/test?characterEncoding=utf8&autoReconnect=true' \
--username root \ 
--password 1234 \ 
--table  sqoop_mysql \ 
--hcatalog-database default \ 
--hcatalog-table app1 \ 
--input-null-string '\\N' \  
--input-null-non-string '\\N' \ 
--num-mappers 4 \ 

create table sqoop_mysql (
vid varchar,
vid varchar,
vcount varchar
) ;

spark on yarn 的工作机制

spark作业提交后会向RM申请启动AM ，在合适的NM分配contain ，启动AM，此时这个AM就相当与driver，
当driver启动之后，会向resourceManager申请Executor内存，RM接到AM的申请后会分配contain，然后在合适的
NM启动executor进程，executor进程启动后会向driver反向注册，Executor全部注册完成后，Driver开始执行行main函数，
之后遇到action算子，触发job，根据宽窄依赖划分stage，每个stage生成对应的taskset，之后将taskset分发到各个executor
上执行，运行完毕释放所有资源

spark提交参数

bin/spark-submit \
--class com.xyz.bigdata.calendar.PeriodCalculator \
--master yarn \
--deploy-mode cluster \
--queue default_queue \
--num-executors 50 \
--executor-cores 2 \
--executor-memory 4G \
--driver-memory 2G \
--conf "spark.default.parallelism=250" \
--conf "spark.shuffle.memoryFraction=0.3" \
--conf "spark.storage.memoryFraction=0.5" \
--conf "spark.driver.extraJavaOptions=-XX:+UseG1GC" \
--conf "spark.executor.extraJavaOptions=-XX:+UseG1GC" \
--verbose \
${PROJECT_DIR}/bigdata-xyz-0.1.jar









